
# üìä Netflix Data Pipeline
Este reposit√≥rio cont√©m um pipeline de dados altamente escal√°vel, automatizado e governado, projetado para garantir ingest√£o, processamento e transforma√ß√£o eficiente dos dados da Netflix. Ele combina tecnologias modernas como Azure Data Factory, Databricks, Delta Live Tables (DLT) e Unity Catalog, proporcionando um ambiente de alta performance, confiabilidade e governan√ßa de dados.

üöÄ Destaques e Benef√≠cios do Projeto

- ‚úÖ Governan√ßa de Dados Centralizada: Utiliza Unity Catalog para controle granular de permiss√µes.
  
- ‚úÖ Data Quality & Observability: Delta Live Tables (DLT) assegura a qualidade dos dados por meio de regras de valida√ß√£o embutidas e monitoramento cont√≠nuo.
  
- ‚úÖ Automa√ß√£o Completa: Jobs no Databricks orquestram todas as etapas do pipeline.
  
- ‚úÖ Escalabilidade e Efici√™ncia: O uso de AutoLoader e Delta Lake otimiza a ingest√£o e o armazenamento, reduzindo custos e tempo de processamento.
  
- ‚úÖ Flexibilidade e Confiabilidade: O pipeline suporta processamento batch e streaming, permitindo ingest√£o em tempo quase real.

## üìå Arquitetura do Projeto
O pipeline segue a arquitetura **Bronze - Silver - Gold**, utilizando ferramentas modernas como **Azure Data Factory (ADF)** para ingest√£o, **AutoLoader para processamento incremental**, **Jobs no Databricks** para orquestra√ß√£o e **DLT para automa√ß√£o e controle de qualidade**.

![image](https://github.com/user-attachments/assets/82981dad-da4d-4b56-b2f9-b63ecfebab4b)

### üîÑ Fluxo de Dados

1. **Azure Data Factory (ADF)** coleta dados do GitHub e os armazena no **Azure Data Lake Gen2**.
2. **AutoLoader no Databricks** l√™ e processa dados novos de forma incremental para a camada **Bronze**.
3. **Transforma√ß√£o na camada Silver**: limpeza, padroniza√ß√£o e enriquecimento dos dados.
4. **Delta Live Tables (DLT)** estrutura e valida os dados na camada **Gold**, garantindo qualidade e conformidade.
5. **Unity Catalog** gerencia as tabelas e os acessos para controle e governan√ßa de dados centralizada.
6. **Os dados processados s√£o disponibilizados para an√°lise** em **Power BI** e **Azure Synapse Analytics**.

---

## üîÑ **Integra√ß√£o com Azure Data Factory**

- **Ingest√£o:** O ADF busca arquivos CSV do GitHub e carrega para o **Data Lake Gen2**.
- **Orquestra√ß√£o:** Dispara os Jobs do Databricks para processar os dados.
- **Monitoramento:** Configurado para alertas de erro via email.
- **Automa√ß√£o do pipeline**, garantindo execu√ß√£o otimizada.

---
## üèóÔ∏è **Unity Catalog e Localiza√ß√µes Externas**

- O **Unity Catalog** centraliza a governan√ßa de dados e fornece controle unificado de acessos.
- Todas as tabelas s√£o gerenciadas dentro do metastore `netflix_unity_metastore`.
- As localiza√ß√µes externas foram configuradas para armazenar os dados no **Azure Data Lake Gen2**, garantindo seguran√ßa e rastreabilidade.

![image](https://github.com/user-attachments/assets/da428e91-2694-4991-ac8d-82378e3e628d)


## üöÄ Notebooks e Processamento de Dados

### 1Ô∏è‚É£ **Camada Bronze - AutoLoader**

Arquivo: `1_autoloader.ipynb`

- O **AutoLoader** faz a ingest√£o autom√°tica e incremental de arquivos CSV.
- Permite escalabilidade para grandes volumes de dados e reduz custos operacionais.
- Detecta automaticamente novos arquivos sem necessidade de monitoramento manual.
- Os dados s√£o armazenados na camada **Bronze** no Data Lake Gen2.

```python
checkpoint_location = "abfss://container@storageaccountl.dfs.core.windows.net/checkpoint"
df = spark.readStream\
    .format('cloudFiles')\
    .option('cloudFiles.format', 'csv')\
    .option('cloudFiles.schemaLocation', checkpoint_location)\
    .load('abfss://raw@storageaccountl.dfs.core.windows.net')
```

### 2Ô∏è‚É£ **Camada Silver - Transforma√ß√µes**

Arquivo: `2_silver.ipynb`

- Realiza limpeza, tratamento de valores nulos e ajuste de tipos de dados.
- Cria√ß√£o de colunas derivadas (`Shorttitle`, `type_flag`, etc.).
- Armazena os dados refinados na camada **Silver** no formato **Delta**.

```python
df = df.withColumn('Shorttitle',split(col('title'),':')[0])
df = df.withColumn('type_flag',when(col('type') == 'Movie',1)\
        .when(col('type') == 'TV Show',2).otherwise(0))
df.write.format('delta')\
     .mode('overwrite')\
     .option('path', 'abfss://container@storageaccountl.dfs.core.windows.net/silver/netflix_titles')\
     .save()
```
## üèóÔ∏è **Jobs do Databricks**

Os **Jobs** no Databricks garantem a automa√ß√£o do pipeline de dados.

### üîπ **Job 1 - Processamento Silver** (`job_silver.json`)
- Executa `3_lookupnotebook.ipynb` para buscar metadados.
- Usa `2_silver.ipynb` para processar diferentes tabelas **Silver** com par√¢metros din√¢micos.
- Executa todas as tabelas do array `my_arr`.
- ![image](https://github.com/user-attachments/assets/e1e5c00b-5880-4568-872f-48517bd75789)

### üîπ **Job 2 - Verifica√ß√£o Condicional**
- Executa `5_lookupNotebook.ipynb` para verificar a data.
- Dependendo do dia da semana, decide qual notebook executar (**4_Silver.ipynb** ou **6_false_notebook.ipynb**).
- ![image](https://github.com/user-attachments/assets/39d092bb-1fdc-471e-a4e9-2ade7e638d86)

### 3Ô∏è‚É£ **Camada Gold - Delta Live Tables (DLT)**

![image](https://github.com/user-attachments/assets/c810bd87-00bf-47a6-bc81-3e2ff1652239)

Arquivo: `7_DLT_Notebook.ipynb`

- **DLT transforma a camada Silver na Gold, aplicando valida√ß√µes e garantindo Data Quality.**
- **Regras de qualidade s√£o aplicadas automaticamente**, rejeitando dados inv√°lidos e garantindo consist√™ncia.
- **Escalabilidade autom√°tica** para grandes volumes de dados.
- **Governan√ßa de dados** com rastreamento de mudan√ßas.


O uso de Delta Live Tables (DLT) torna este pipeline altamente eficiente, garantindo qualidade de dados desde a ingest√£o at√© a camada de consumo.

#### **Vantagens do Delta Live Tables (DLT):**
- ‚úÖ **Automa√ß√£o total do pipeline de dados** ‚Äì n√£o precisa gerenciar tarefas manualmente.
- ‚úÖ **Verifica√ß√£o e valida√ß√£o de qualidade embutida** com `@dlt.expect_all_or_drop()`.
- ‚úÖ **Hist√≥rico completo das altera√ß√µes** ‚Äì facilita auditorias e conformidade.
- ‚úÖ **Execu√ß√£o otimizada e escal√°vel**, reduzindo custos operacionais.


A escalabilidade e o processamento otimizado do Databricks reduzem o tempo de execu√ß√£o e otimizam os custos de armazenamento e computa√ß√£o.Com rastreabilidade total das transforma√ß√µes, o pipeline mant√©m um hist√≥rico completo das mudan√ßas nos dados, facilitando auditorias e an√°lises.









