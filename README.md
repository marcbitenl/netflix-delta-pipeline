
# üìä Netflix Data Pipeline

![image](https://github.com/user-attachments/assets/82981dad-da4d-4b56-b2f9-b63ecfebab4b)

Este reposit√≥rio cont√©m um pipeline de dados completo para ingest√£o, processamento e transforma√ß√£o de informa√ß√µes da Netflix utilizando **Azure Data Factory, Databricks, Delta Live Tables (DLT) e Unity Catalog**.

## üìå Arquitetura do Projeto

O pipeline segue a arquitetura **Bronze - Silver - Gold**, utilizando ferramentas modernas como **Azure Data Factory (ADF)** para ingest√£o, **AutoLoader para processamento incremental**, **Jobs no Databricks** para orquestra√ß√£o e **DLT para automa√ß√£o e controle de qualidade**.

üì∑ **Fluxo Geral do Pipeline:**



### üîÑ Fluxo de Dados

1. **Azure Data Factory (ADF)** coleta dados do GitHub e os armazena no **Azure Data Lake Gen2**.
2. **AutoLoader no Databricks** l√™ e processa dados novos de forma incremental para a camada **Bronze**.
3. **Transforma√ß√£o na camada Silver**: limpeza, padroniza√ß√£o e enriquecimento dos dados.
4. **Delta Live Tables (DLT)** estrutura e valida os dados na camada **Gold**, garantindo qualidade e conformidade.
5. **Unity Catalog** gerencia as tabelas e os acessos para controle e governan√ßa de dados centralizada.
6. **Os dados processados s√£o disponibilizados para an√°lise** em **Power BI** e **Azure Synapse Analytics**.

---

## üîÑ **Integra√ß√£o com Azure Data Factory**

Arquivo: `pipeline_adf.json`

- **Ingest√£o:** O ADF busca arquivos CSV do GitHub e carrega para o **Data Lake Gen2**.
- **Orquestra√ß√£o:** Dispara os Jobs do Databricks para processar os dados.
- **Monitoramento:** Configurado para alertas de erro via email.
- **Automa√ß√£o do pipeline**, garantindo execu√ß√£o otimizada.

---

## üöÄ Notebooks e Processamento de Dados

### 1Ô∏è‚É£ **Camada Bronze - AutoLoader**

Arquivo: `1_autoloader.ipynb`

- O **AutoLoader** faz a ingest√£o autom√°tica e incremental de arquivos CSV.
- Permite escalabilidade para grandes volumes de dados e reduz custos operacionais.
- Detecta automaticamente novos arquivos sem necessidade de monitoramento manual.
- Os dados s√£o armazenados na camada **Bronze** no Data Lake Gen2.

```python
checkpoint_location = "abfss://container@storageaccountl.dfs.core.windows.net/checkpoint"
df = spark.readStream\
    .format('cloudFiles')\
    .option('cloudFiles.format', 'csv')\
    .option('cloudFiles.schemaLocation', checkpoint_location)\
    .load('abfss://raw@storageaccountl.dfs.core.windows.net')
```

### 2Ô∏è‚É£ **Camada Silver - Transforma√ß√µes**

Arquivo: `2_silver.ipynb`

- Realiza limpeza, tratamento de valores nulos e ajuste de tipos de dados.
- Cria√ß√£o de colunas derivadas (`Shorttitle`, `type_flag`, etc.).
- Armazena os dados refinados na camada **Silver** no formato **Delta**.

```python
df = df.withColumn('Shorttitle',split(col('title'),':')[0])
df = df.withColumn('type_flag',when(col('type') == 'Movie',1)\
        .when(col('type') == 'TV Show',2).otherwise(0))
df.write.format('delta')\
     .mode('overwrite')\
     .option('path', 'abfss://container@storageaccountl.dfs.core.windows.net/silver/netflix_titles')\
     .save()
```

### 3Ô∏è‚É£ **Camada Gold - Delta Live Tables (DLT)**

![image](https://github.com/user-attachments/assets/c810bd87-00bf-47a6-bc81-3e2ff1652239)


Arquivo: `7_DLT_Notebook.ipynb`

- **DLT transforma a camada Silver na Gold, aplicando valida√ß√µes e garantindo Data Quality.**
- **Regras de qualidade s√£o aplicadas automaticamente**, rejeitando dados inv√°lidos e garantindo consist√™ncia.
- **Escalabilidade autom√°tica** para grandes volumes de dados.
- **Governan√ßa de dados** com rastreamento de mudan√ßas.

#### **Vantagens do Delta Live Tables (DLT):**
- ‚úÖ **Automa√ß√£o total do pipeline de dados** ‚Äì n√£o precisa gerenciar tarefas manualmente.
- ‚úÖ **Verifica√ß√£o e valida√ß√£o de qualidade embutida** com `@dlt.expect_all_or_drop()`.
- ‚úÖ **Hist√≥rico completo das altera√ß√µes** ‚Äì facilita auditorias e conformidade.
- ‚úÖ **Execu√ß√£o otimizada e escal√°vel**, reduzindo custos operacionais.

```python
@dlt.table(
  name = "gold_netflixdirectors"
)
@dlt.expect_all_or_drop({'rule1': 'show_id is NOT NULL'})
def gold_netflixdirectors():
    df = spark.readStream.format('delta').load('abfss://container@storageaccountl.dfs.core.windows.net/silver/netflix_directors')
    return df
```

---

## üèóÔ∏è **Unity Catalog e Localiza√ß√µes Externas**

- O **Unity Catalog** centraliza a governan√ßa de dados e fornece controle unificado de acessos.
- Todas as tabelas s√£o gerenciadas dentro do metastore `netflix_unity_metastore`.
- As localiza√ß√µes externas foram configuradas para armazenar os dados no **Azure Data Lake Gen2**, garantindo seguran√ßa e rastreabilidade.

üì∑ **Print do Unity Catalog:**

![image](https://github.com/user-attachments/assets/da428e91-2694-4991-ac8d-82378e3e628d)


---

## üèóÔ∏è **Jobs do Databricks**

Os **Jobs** no Databricks garantem a automa√ß√£o do pipeline de dados.

### üîπ **Job 1 - Processamento Silver** (`job_silver.json`)
- Executa `3_lookupnotebook.ipynb` para buscar metadados.
- Usa `2_silver.ipynb` para processar diferentes tabelas **Silver** com par√¢metros din√¢micos.
- Executa todas as tabelas do array `my_arr`.
- ![image](https://github.com/user-attachments/assets/e1e5c00b-5880-4568-872f-48517bd75789)

  

### üîπ **Job 2 - Verifica√ß√£o Condicional**
- Executa `5_lookupNotebook.ipynb` para verificar a data.
- Dependendo do dia da semana, decide qual notebook executar (**4_Silver.ipynb** ou **6_false_notebook.ipynb**).
- ![image](https://github.com/user-attachments/assets/39d092bb-1fdc-471e-a4e9-2ade7e638d86)


---


